{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import simpleDataset, uniformDataset, contextUniformDataset\n",
    "from model import MLP\n",
    "from train import train_model, evaluate_model\n",
    "from test import test_model\n",
    "from utils import *\n",
    "import os\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "num_workers = 4 if cuda else 0 \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch version: %s\" % torch.__version__)\n",
    "print(\"np version: %s\" % np.__version__)\n",
    "print(\"__debug__: %s\" % __debug__)\n",
    "print(\"cuda: %s\" % cuda)\n",
    "print(\"num_workers: %s\" % num_workers)\n",
    "print(\"device: %s\" % device)\n",
    "print(\"verbose: %s\" % verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "data_path = \"../data\"\n",
    "checkpoint_path = \"../checkpoint\"\n",
    "\n",
    "path_trainx = \"%s/train.npy\" % data_path\n",
    "path_trainy = \"%s/train_labels.npy\" % data_path\n",
    "path_evalx = \"%s/dev.npy\" % data_path\n",
    "path_evaly = \"%s/dev_labels.npy\" % data_path\n",
    "path_testx = \"%s/test.npy\" % data_path\n",
    "\n",
    "pred_filename = \"%s/test_pred.csv\" % data_path\n",
    "checkpoint_filename = \"%s/checkpoint_batchnorm.tar\" % checkpoint_path\n",
    "\n",
    "checkpoint = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalx = np.load(path_evalx, allow_pickle=True)\n",
    "evaly = np.load(path_evaly, allow_pickle=True)\n",
    "\n",
    "trainx = np.load(path_trainx, allow_pickle=True) # Note: for real training\n",
    "trainy = np.load(path_trainy, allow_pickle=True) # Note: for real training\n",
    "# trainx = evalx # Note: for development\n",
    "# trainy = evaly # Note: for development\n",
    "\n",
    "testx = np.load(path_testx, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "orig_x_dim, orig_y_dim = trainx[0].shape[1], 138\n",
    "context_size = 5\n",
    "input_size, output_size = orig_x_dim * (2*context_size+1), orig_y_dim\n",
    "size_list = [input_size, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, output_size]\n",
    "activation = \"relu\" # use only one type of activation\n",
    "lr = 1e-3 # default lr is 1e-3\n",
    "epochs = 15\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "print(\"loading data...\")\n",
    "\n",
    "train_dataset = contextUniformDataset(trainx, trainy, context_size=context_size)\n",
    "eval_dataset = contextUniformDataset(evalx, evaly, context_size=context_size) # for real training\n",
    "# eval_dataset = train_dataset # for development\n",
    "test_dataset = contextUniformDataset(testx, is_test=True, context_size=context_size)\n",
    "\n",
    "# train_dataset = simpleDataset(trainx, trainy, context_size=context_size)\n",
    "# eval_dataset = simpleDataset(evalx, evaly, context_size=context_size) # for real training\n",
    "\n",
    "if verbose:\n",
    "    print(\"train size: X:({}, {}) Y:({}, 1)\".format(len(train_dataset), len(train_dataset[0][0]), len(train_dataset)))\n",
    "    print(\"eval size: X:({}, {}) Y:({}, 1)\".format(len(eval_dataset), len(eval_dataset[0][0]), len(eval_dataset)))\n",
    "#     print(\"test size: X:({}, {})\".format(len(test_dataset), len(test_dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, # The dataset\n",
    "    batch_size=batch_size,      # Batch size\n",
    "    shuffle=True,      # Shuffles the dataset at every epoch\n",
    "    pin_memory=True,   # Copy data to CUDA pinned memory\n",
    "    num_workers=num_workers      # Number of worker processes for loading data.\n",
    "                       )\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=num_workers\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "print(\"building model...\")\n",
    "model = MLP(size_list, activation)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.to(device).parameters(), lr=1e-3)\n",
    "\n",
    "# check if load checkpoint\n",
    "if not os.path.exists(checkpoint_path) or not os.path.exists(checkpoint_filename):\n",
    "    print(\"checkpoint path: %s does not exist. Initialize new checkpoint\" % checkpoint_filename)\n",
    "    init_checkpoint(checkpoint)\n",
    "\n",
    "elif load_model and os.path.exists(checkpoint_filename):\n",
    "    print(\"checkpoint file %s exist. Be careful about overwriting checkpoint! \"\n",
    "          \"Load checkpoint\" % checkpoint_filename)\n",
    "    checkpoint = load_checkpoint(checkpoint_filename, model, optimizer)\n",
    "\n",
    "print_model_statistics(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "print(\"training...\")\n",
    "train_losses, eval_losses, eval_accs = \\\n",
    "    train_model(model, epochs, train_loader, eval_loader, criterion, optimizer, \n",
    "                device, checkpoint=checkpoint, checkpoint_filename=checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predicting...\")\n",
    "predicts = test_model(model, test_loader, device, save=True, filename=pred_filename)\n",
    "print(\"finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
